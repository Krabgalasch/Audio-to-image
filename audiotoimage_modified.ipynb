{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing packages\n",
    "#pip install faster_whisper diffusers transformers accelerate xformers scipy sounddevice pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All requiered Imports\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "from pydub import AudioSegment\n",
    "#from pydub.utils import which\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record audio using microphone\n",
    "\n",
    "\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(r\"D:\\soundtoimage\")\n",
    "\n",
    "def record_audio(duration, sample_rate=44100):\n",
    "    print(\"Recording...\")\n",
    "    audio_data = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype='int16')\n",
    "    sd.wait()  # Wait until recording is finished\n",
    "    print(\"Recording complete. Saving as WAV...\")\n",
    "    write(\"temp.wav\", sample_rate, audio_data)  # Save as WAV file\n",
    "\n",
    "    # Verify if WAV file was created\n",
    "    if os.path.exists(\"temp.wav\"):\n",
    "        print(\"temp.wav file found.\")\n",
    "        return\n",
    "    \n",
    "    #print(AudioSegment.converter)\n",
    "\n",
    "# Record a 5-second audio and save as 'recorded.mp3'\n",
    "record_audio(duration=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_file_path = \"temp.wav\" # Set the path for the wav file\n",
    "print(os.path.exists(wav_file_path))  # Should print True if the file exists\n",
    "\n",
    "\n",
    "\n",
    "#Convert the audio file to segments\n",
    "def convert_audio(filename):\n",
    "    # Convert WAV to MP3\n",
    "    print(\"Converting to MP3...\")\n",
    "    try:\n",
    "        audio = AudioSegment.from_wav(\"temp.wav\")\n",
    "        audio.export(filename, format=\"mp3\")\n",
    "        print(f\"Saved as {filename}.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(\"Error: File not found.\")\n",
    "        print(e)\n",
    "\n",
    "mp3_file = convert_audio(\"recorded.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_str(s):\n",
    "\treturn s.replace('\\'', '\\\\\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Cuda availabillity\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "model = WhisperModel('medium', compute_type='int8', device=\"cpu\") # load Whisper model (text transcription)\n",
    "segments, info = model.transcribe(\"temp.wav\", word_timestamps=True) # get transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# segment phrases better - general logic is to further split phrases where there is punctuation or pauses in speaking\n",
    "SPEAK_PAUSE = 1 # time in seconds for a word to be spoken before it is considered a pause\n",
    "transcript = ''\n",
    "segment_timestamps, word_timestamps = [], []\n",
    "start = None\n",
    "segment_str = ''\n",
    "\n",
    "for s in segments:\n",
    "    transcript += s.text\n",
    "    for idx, word in enumerate(s.words):\n",
    "        word_timestamps.append([word.start, word.end, word.word])\n",
    "        segment_str = segment_str + word.word\n",
    "\n",
    "        if start is None:\n",
    "            start = word.start\n",
    "        if (word.word[-1] in ',!.;?') or (word.end - word.start > SPEAK_PAUSE) or ((idx == len(s.words) - 1) and start is not None):\n",
    "            if segment_str != '':\n",
    "                segment_timestamps.append([start, word.end, segment_str.strip()])\n",
    "            segment_str = ''\n",
    "            start = None\n",
    "\n",
    "print('Transcript')\n",
    "print(transcript)\n",
    "\n",
    "print('Automatically generated word timestamps')\n",
    "for s, e, w in word_timestamps:\n",
    "    print(s, e, w)\n",
    "print()\n",
    "\n",
    "print('Automatically generated segment timestamps')\n",
    "for s, e, w in segment_timestamps:\n",
    "    print(s, e, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [segment for _s, _e, segment in segment_timestamps]\n",
    "\n",
    "print('Current Prompts')\n",
    "print('[')\n",
    "for p in prompts:\n",
    "    print(f'\\t\\'{safe_str(p)}\\',')\n",
    "print(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test image generation for the prompts to experiment with prompts for Stable Diffusion\n",
    "\n",
    "# Load the Stable Diffusion pipeline\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None  # Safety checker disabled; ensure ethical use\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")  # Move pipeline to GPU\n",
    "\n",
    "# Define test prompts\n",
    "test_prompts = [\n",
    "    'Goat,',\n",
    "    'animal,',\n",
    "    'water,',\n",
    "    'the goat man is',\n",
    "    'jumping from the city skyline,',\n",
    "    'Jesus Christ,',\n",
    "    'keine',\n",
    "    'Ahnung,',\n",
    "    'ich frag mich ob es auch in beiden Sprachen geht,',\n",
    "    'ich nehme einfach eine 20',\n",
    "    'Sekunden Audio gerade auf mit irgendwelchen Worten,',\n",
    "    'Obama,',\n",
    "    'Bush,',\n",
    "    'nein',\n",
    "]\n",
    "\n",
    "if not test_prompts:\n",
    "    raise ValueError(\"No prompts provided for testing.\")\n",
    "\n",
    "print('Current Testing Prompts (test images will be generated):')\n",
    "print('[')\n",
    "for p in test_prompts:\n",
    "    print(f'\\t\\'{p}\\',')\n",
    "print(']')\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# Optimize pipeline\n",
    "pipe.enable_attention_slicing()\n",
    "pipe.unet.to(memory_format=torch.channels_last)\n",
    "\n",
    "# Attempt to enable xformers, with fallback if unsupported\n",
    "try:\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "except NotImplementedError as e:\n",
    "    print(\"xformers not supported, proceeding without it:\", e)\n",
    "\n",
    "# Divide prompts into batches\n",
    "prompt_chunks = [\n",
    "    test_prompts[batch_size * s_i : batch_size * (s_i + 1)]\n",
    "    for s_i in range(len(test_prompts) // batch_size)\n",
    "]\n",
    "if len(test_prompts) % batch_size != 0:\n",
    "    prompt_chunks.append(test_prompts[-(len(test_prompts) % batch_size):])\n",
    "\n",
    "# Generate images for each batch of prompts\n",
    "images = []\n",
    "for ps in tqdm(prompt_chunks, desc=\"Generating Images\"):\n",
    "    images += pipe(ps, num_inference_steps=20, guidance_scale=8.5).images\n",
    "\n",
    "# Display generated images\n",
    "for prompt, image in zip(test_prompts, images):\n",
    "    plt.imshow(image)\n",
    "    plt.title(prompt)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get timestamps for stable diffusion prompts\n",
    "prompt_transcript_pairs = [ # each element is the prompt string (or (prompt string, string within transcript that corresponds with your prompt))\n",
    "    # PUT PAIRS HERE \n",
    "    # Use Mood for first parameter(mood,prompt)\n",
    "    ('Goat','Goat'),\n",
    "    ('animal','animal'),\n",
    "    ('water','water'),\n",
    "    ('goat man','the goat man is'),\n",
    "    ('jumping from the city skyline','jumping from the city skyline'),\n",
    "    ('Jesus Christ','Jesus Christ'),\n",
    "    ('keine','keine'),\n",
    "    ('beiden Sprachen','ich frag mich ob es auch in beiden Sprachen geht,'),\n",
    "    ('nehme einfach eine 20','ich nehme einfach eine 20'),\n",
    "    ('irgendwelchen Worten','Sekunden Audio gerade auf mit irgendwelchen Worten,'),\n",
    "    ('Obama','Obama,'),\n",
    "    ('Bush','Bush,'),\n",
    "    ('nein','nein')\n",
    "]\n",
    "\n",
    "\n",
    "to_alnum = lambda s: re.sub(r'[^a-zA-Z0-9_\\s]+', '', s)\n",
    "transcript_words = to_alnum(transcript.lower()).split()\n",
    "print('TEXT TRANSCRIPT')\n",
    "start_word_idx = 0\n",
    "for p_idx, p in enumerate(prompt_transcript_pairs):\n",
    "    if isinstance(p, str):\n",
    "        transcript_text = p\n",
    "    else:\n",
    "        p, transcript_text = p\n",
    "\n",
    "    for search_word_idx, word in enumerate(to_alnum(transcript_text).split()):\n",
    "        try:\n",
    "            word_idx = transcript_words.index(word.lower(), start_word_idx)\n",
    "        except IndexError:\n",
    "            raise IndexError(f'{word} not found in transcript; prompt: {p}')\n",
    "        if search_word_idx == 0:\n",
    "            start_word_idx = word_idx\n",
    "\n",
    "        if word_idx != start_word_idx + search_word_idx:\n",
    "            break\n",
    "    else:\n",
    "        s_time = 0 if p_idx == 0 else word_timestamps[start_word_idx][0]\n",
    "        print(f'({s_time}, \\'{safe_str(p)})\\',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get timestamps for stable diffusion prompts\n",
    "prompt_transcript_pairs = [ # each element is the prompt string (or (prompt string, string within transcript that corresponds with your prompt))\n",
    "    # PUT PAIRS HERE \n",
    "    # Use Mood for first parameter(mood,prompt)\n",
    "    ('','Goat'),\n",
    "    ('','animal'),\n",
    "    ('','water'),\n",
    "    ('','the goat man is')\n",
    "    ('','jumping from the city skyline'),\n",
    "    ('','Jesus Christ'),\n",
    "    ('','keine'),\n",
    "    ('','ich frag mich ob es auch in beiden Sprachen geht,'),\n",
    "    ('','ich nehme einfach eine 20'),\n",
    "    ('','Sekunden Audio gerade auf mit irgendwelchen Worten,'),\n",
    "    ('','Obama,'),\n",
    "    ('','Bush,'),\n",
    "    ('','nein'),\n",
    "]\n",
    "\n",
    "\n",
    "to_alnum = lambda s: re.sub(r'[^a-zA-Z0-9_\\s]+', '', s)\n",
    "transcript_words = to_alnum(transcript.lower()).split()\n",
    "print('TEXT TRANSCRIPT')\n",
    "start_word_idx = 0\n",
    "for p_idx, p in enumerate(prompt_transcript_pairs):\n",
    "    if isinstance(p, str):\n",
    "        transcript_text = p\n",
    "    else:\n",
    "        p, transcript_text = p\n",
    "\n",
    "    for search_word_idx, word in enumerate(to_alnum(transcript_text).split()):\n",
    "        try:\n",
    "            word_idx = transcript_words.index(word.lower(), start_word_idx)\n",
    "        except IndexError:\n",
    "            raise IndexError(f'{word} not found in transcript; prompt: {p}')\n",
    "        if search_word_idx == 0:\n",
    "            start_word_idx = word_idx\n",
    "\n",
    "        if word_idx != start_word_idx + search_word_idx:\n",
    "            break\n",
    "    else:\n",
    "        s_time = 0 if p_idx == 0 else word_timestamps[start_word_idx][0]\n",
    "        print(f'({s_time}, \\'{safe_str(p)})\\',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "classifier = EncoderClassifier.from_hparams(\n",
    "    source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "    savedir=\"tmpdir\"\n",
    ")\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.pretrained import SpeakerRecognition\n",
    "\n",
    "# Function to clean text to alphanumeric\n",
    "\n",
    "def to_alnum(s):\n",
    "    return re.sub(r'[^a-zA-Z0-9_\\s]+', '', s)\n",
    "\n",
    "# Mood detection function using SpeechBrain\n",
    "def detect_mood(audio_file):\n",
    "    # Load pre-trained model\n",
    "    emotion_model = SpeakerRecognition.from_hparams(source=\"speechbrain/emotion-recognition\", savedir=\"tmp\")\n",
    "\n",
    "    # Load the audio file\n",
    "    signal, sr = librosa.load(audio_file, sr=16000)  # Ensure 16kHz sampling rate\n",
    "    duration = librosa.get_duration(signal, sr=sr)\n",
    "\n",
    "    # Process the audio in chunks (e.g., 2-second chunks)\n",
    "    chunk_duration = 2  # seconds\n",
    "    mood_list = []\n",
    "    for start in range(0, int(duration), chunk_duration):\n",
    "        end = min(int(duration), start + chunk_duration)\n",
    "        chunk = signal[int(start * sr):int(end * sr)]\n",
    "\n",
    "        # Save the chunk temporarily for processing\n",
    "        chunk_file = f\"chunk_{start}_{end}.wav\"\n",
    "        librosa.output.write_wav(chunk_file, chunk, sr)\n",
    "\n",
    "        # Run emotion detection\n",
    "        prediction = emotion_model.classify_file(chunk_file)\n",
    "        emotion = prediction['class']  # Extract predicted emotion\n",
    "\n",
    "        # Map emotions to moods\n",
    "        mood_mapping = {\n",
    "            \"happy\": \"energetic\",\n",
    "            \"excited\": \"energetic\",\n",
    "            \"sad\": \"melancholic\",\n",
    "            \"angry\": \"intense\",\n",
    "            \"neutral\": \"neutral\",\n",
    "            \"calm\": \"calm\"\n",
    "        }\n",
    "        mood = mood_mapping.get(emotion.lower(), \"unknown\")\n",
    "        mood_list.append(mood)\n",
    "\n",
    "    return mood_list\n",
    "\n",
    "# Load Stable Diffusion pipeline\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None  # Safety checker disabled; ensure ethical use\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "# Define prompts\n",
    "prompts = [\n",
    "    'Goat',\n",
    "    'animal',\n",
    "    'water',\n",
    "    'the goat man is',\n",
    "    'jumping from the city skyline',\n",
    "    'Jesus Christ',\n",
    "    'keine',\n",
    "    'ich frag mich ob es auch in beiden Sprachen geht',\n",
    "    'ich nehme einfach eine 20',\n",
    "    'Sekunden Audio gerade auf mit irgendwelchen Worten',\n",
    "    'Obama',\n",
    "    'Bush',\n",
    "    'nein',\n",
    "]\n",
    "\n",
    "# Define transcript and extract moods\n",
    "transcript = \"This is your audio transcript with sentences corresponding to prompts.\"  # Replace with actual transcript\n",
    "word_timestamps = []  # Replace with word-level timestamps if available\n",
    "\n",
    "audio_file = \"your_audio_file.wav\"  # Replace with the actual audio file path\n",
    "moods = detect_mood(audio_file)\n",
    "\n",
    "# Ensure moods match the number of prompts\n",
    "if len(moods) < len(prompts):\n",
    "    moods.extend([\"neutral\"] * (len(prompts) - len(moods)))  # Pad with neutral if necessary\n",
    "elif len(moods) > len(prompts):\n",
    "    moods = moods[:len(prompts)]  # Trim excess moods\n",
    "\n",
    "# Create prompt-transcript pairs\n",
    "prompt_transcript_pairs = [(mood, prompt) for mood, prompt in zip(moods, prompts)]\n",
    "\n",
    "# Output prompt-transcript pairs\n",
    "print(\"Prompt-Transcript Pairs:\")\n",
    "for pair in prompt_transcript_pairs:\n",
    "    print(pair)\n",
    "\n",
    "# Batch size and attention optimization\n",
    "batch_size = 4\n",
    "pipe.enable_attention_slicing()\n",
    "pipe.unet.to(memory_format=torch.channels_last)\n",
    "\n",
    "# Attempt to enable xformers memory-efficient attention\n",
    "try:\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "except NotImplementedError as e:\n",
    "    print(\"xformers is not fully supported, proceeding without it:\", e)\n",
    "\n",
    "# Generate images in batches\n",
    "prompt_chunks = [\n",
    "    prompts[batch_size * s_i: batch_size * (s_i + 1)]\n",
    "    for s_i in range(len(prompts) // batch_size)\n",
    "]\n",
    "if len(prompts) % batch_size != 0:\n",
    "    prompt_chunks.append(prompts[-(len(prompts) % batch_size):])\n",
    "\n",
    "images = []\n",
    "for ps in tqdm(prompt_chunks, desc=\"Generating Images\"):\n",
    "    images += pipe(ps, num_inference_steps=20, guidance_scale=8.5).images\n",
    "\n",
    "# Display generated images\n",
    "for prompt, image in zip(prompts, images):\n",
    "    plt.imshow(image)\n",
    "    plt.title(prompt)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiotoimage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
